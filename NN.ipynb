{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementando neurônios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Vamos implementar o neurônio destacado\n",
    "\n",
    "![REDE NEURAL](https://i.imgur.com/eUu71E1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#vamos supor que estamos fazendo um neurônio que esteja implementado no meio de uma rede neural, com conexões de entrada e saída nele.\n",
    "#nesse caso, todos os neurônios tem conexões com cada neurônio prévio a ele, vamos supor que tenham 3 neurônios conectados que estejam\n",
    "#dando entrada nesse neurônio que vamos fazer.\n",
    "\n",
    "#esses inputs são as saidas dos três neurônios previos\n",
    "inputs = [1, 2, 3]\n",
    "\n",
    "#cada input tem um peso associado a ele\n",
    "weights = [0.2, 0.8, -0.5]\n",
    "\n",
    "#cada neurônio tem um bias único, como estamos fazendo um único, neurônio, só há 1 bias\n",
    "bias = 2\n",
    "\n",
    "#a saida do neurônio é a soma dos produtos de cada dado de entrada com seu respectivo peso com o bias\n",
    "output = inputs[0]*weights[0] + inputs[1]*weights[1] + inputs[2]*weights[2] + bias\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nosso neurônio ficou com essa cara\n",
    "\n",
    "![NEURONIO](https://i.imgur.com/DB6FAYC.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Agora vamos modelar um neurônio da camada de saída:\n",
    " \n",
    "![REDE NEURAL](https://i.imgur.com/NEByrWc.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.8"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#A mesma coisa do outro neurônio, a diferença é que agora o neurônio está tendo 4 entradas de uma camada oculta.\n",
    "\n",
    "#esses inputs são as saidas dos quatro neurônios previos\n",
    "inputs = [1, 2, 3, 2.5]\n",
    "\n",
    "#cada input tem um peso associado a ele\n",
    "weights = [0.2, 0.8, -0.5, 1.0]\n",
    "\n",
    "#cada neurônio tem um bias único\n",
    "bias = 2\n",
    "\n",
    "#a saida do neurônio é a soma dos produtos de cada dado de entrada com seu respectivo peso com o bias\n",
    "output = inputs[0]*weights[0] + inputs[1]*weights[1] + inputs[2]*weights[2] + inputs[3]*weights[3] + bias\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nosso neurônio ficou assim:\n",
    "\n",
    "![NEURONIO](https://i.imgur.com/HzH98wE.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementando camadas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos implementar a camada de saida:\n",
    "\n",
    "![REDE NEURAL](https://i.imgur.com/GDsbq06.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.8, 1.21, 2.385]\n"
     ]
    }
   ],
   "source": [
    "#Sao 3 neuronios de saida, note que cada um tem 4 entradas\n",
    "#A lógica dos neurônios continua sendo a mesma que já foi implementada.\n",
    "\n",
    "#esses inputs são as saidas dos quatro neurônios previos\n",
    "inputs = [1, 2, 3, 2.5]\n",
    "\n",
    "#cada input tem um peso associado a ele, como sao 3 neurônios, teremos 3 listas de pesos\n",
    "weights = [\n",
    "    [0.2, 0.8, -0.5, 1.0],\n",
    "    [0.5, -0.91, 0.26, -0.5],\n",
    "    [-0.26, -0.27, 0.17, 0.87]\n",
    "]\n",
    "\n",
    "#cada neurônio tem um bias único (menos os da camada de entrada, entao serao 3 biases)\n",
    "biases = [2, 3, 0.5]\n",
    "\n",
    "#a saida do neurônio é a soma dos produtos de cada dado de entrada com seu respectivo peso com o bias\n",
    "#saida = entrada * peso + bias \n",
    "\n",
    "#essa funcao percorre a lista de pesos e biases e nos retorna as saidas dos neuronios\n",
    "#resultado_camada é a lista em que vamos guardar o resultado de cada camada\n",
    "resultado_camada = []\n",
    "#o que o zip() faz é combinar duas listas em uma, formando uma lista de tuplas, \n",
    "#assim dá pra percorrer as duas listas paralelamente\n",
    "for peso_neuronio, bias_neuronio in zip(weights, biases):\n",
    "    #inicializa a saida do neuronio como 0\n",
    "    saida_neuronio = 0\n",
    "    #aqui de novo ele faz o zip, so que agora com os inputs do neuronio e o peso,\n",
    "    #que foi usado no zip acima \n",
    "    for entrada_neuronio, peso in zip(inputs, peso_neuronio):\n",
    "        #aqui fazemos o processo para calcular a saida do neuronio, multiplicando as \n",
    "        #entradas e seus respectivos pesos\n",
    "        saida_neuronio += entrada_neuronio*peso\n",
    "    #aqui ele pega o resultado da multiplicacao e soma com o bias do neuronio\n",
    "    saida_neuronio += bias_neuronio\n",
    "    #acrescenta a saida do neuronio na lista do resultado das camadas\n",
    "    resultado_camada.append(saida_neuronio)\n",
    "\n",
    "print(resultado_camada)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O primeiro neurônio ficou assim:\n",
    "\n",
    "![REDE NEURAL](https://i.imgur.com/Bi6fovb.png)\n",
    "\n",
    "![REDE NEURAL](https://i.imgur.com/TonkYba.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usando o Dot Product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exemplo de uso em um único neurônio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.8"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dot product é a função do numpy que faz multiplicação entre vetores\n",
    "import numpy as np\n",
    "\n",
    "#vamos simplificar para esse teste para um neurônio só\n",
    "inputs = [1, 2, 3, 2.5]\n",
    "weights = [0.2, 0.8, -0.5, 1.0]\n",
    "bias = 2\n",
    "\n",
    "#na função passamos o array A, depois o array B\n",
    "resultado = np.dot(inputs, weights) + bias\n",
    "\n",
    "resultado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exemplo de uso em uma camada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.8  , 1.21 , 2.385])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "inputs = [1, 2, 3, 2.5]\n",
    "\n",
    "weights = [\n",
    "    [0.2, 0.8, -0.5, 1.0],\n",
    "    [0.5, -0.91, 0.26, -0.5],\n",
    "    [-0.26, -0.27, 0.17, 0.87]\n",
    "]\n",
    "\n",
    "biases = [2, 3, 0.5]\n",
    "\n",
    "#A ORDEM QUE PASSAMOS ESSES ARRAY IMPORTA NESSE CASO,\n",
    "#O PRIMEIRO ELEMENTO QUE VC PASSA É A FORMA COMO O \n",
    "#RETORNO VAI SER INDEXADO,\n",
    "#COMO TEMOS TRÊS NEURÔNIOS, QUEREMOS QUE ELES \n",
    "#SEJAM INDEXADOS POR ESSES TRÊS SETS DE PESOS\n",
    "\n",
    "resultado = np.dot(weights, inputs) + biases\n",
    "\n",
    "resultado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemplo de uso de entradas em lotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.8  ,  9.9  , -0.09 ],\n",
       "       [ 0.21 , -1.81 , -1.449],\n",
       "       [ 3.885,  2.7  ,  0.026]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#agora estamos passando as entradas em lotes, mas o cenário da rede neural continuaria o mesmo,\n",
    "#seriam três neurônios enviando suas entradas em lotes \n",
    "inputs = [[1, 2, 3, 2.5],\n",
    "          [2.0, 5.0, -1.0, 2.0],\n",
    "          [-1.5, 2.7, 3.3, -0.8]]\n",
    "\n",
    "weights = [\n",
    "    [0.2, 0.8, -0.5, 1.0],\n",
    "    [0.5, -0.91, 0.26, -0.5],\n",
    "    [-0.26, -0.27, 0.17, 0.87]\n",
    "]\n",
    "\n",
    "biases = [2, 3, 0.5]\n",
    "\n",
    "#agora temos que pegar e fazer a transposta das entradas, então pegamos e transformamos os inputs em um array do numpy e usamos o .T para transposta\n",
    "#fazemos isso porque antes, os pesos eram uma matriz e as entradas eram um simples vetor, o shape da matriz era (3,4) - 3 linhas com 4 elementos em cada e o shape do vetor era\n",
    "#(,4) - 1 dimensão com 4 elementos, nesse sentido o .dot funcionava, agora com os lotes, o shape da matriz de pesos é (3,4) e o da matriz de entrada também é (3,4), no dotproduct\n",
    "#isso não vai servir porque temos que ter o número de colunas da primeira (4) tem que ser igual ao numero de linhas da segunda, fazendo a transposta, a matriz que antes era (3,4)\n",
    "#passa a ser (4,3) o que faz com que o número de colunas da primeira (4), fiquem igual ao numero de linhas da segunda (4). Nisso a matriz formada é uma com o número de linhas da primeira\n",
    "#e o número de colunas da segunda, nesse caso, será uma matriz 3 x 3.\n",
    "resultado = np.dot(weights, np.array(inputs).T) + biases\n",
    "\n",
    "#o bias é adicionada em cada linha, então ele pega a linha 1 da matriz resultante e adiciona em cada elemento o bias daquela posição.\n",
    "resultado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemplo de uso de lotes em mais de uma camada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.8    9.9   -0.09 ]\n",
      " [ 0.21  -1.81  -1.449]\n",
      " [ 3.885  2.7    0.026]]\n",
      "[[-0.35   2.64  -1.89 ]\n",
      " [-3.98  -0.27  -0.763]\n",
      " [ 1.805  6.64   1.006]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#agora estamos passando as entradas em lotes, mas o cenário da rede neural continuaria o mesmo,\n",
    "#seriam três neurônios enviando suas entradas em lotes \n",
    "inputs = [[1, 2, 3, 2.5],\n",
    "          [2.0, 5.0, -1.0, 2.0],\n",
    "          [-1.5, 2.7, 3.3, -0.8]]\n",
    "\n",
    "#na primeira camada vamos ter \n",
    "weights = [\n",
    "    [0.2, 0.8, -0.5, 1.0],\n",
    "    [0.5, -0.91, 0.26, -0.5],\n",
    "    [-0.26, -0.27, 0.17, 0.87]\n",
    "]\n",
    "\n",
    "biases = [2, 3, 0.5]\n",
    "\n",
    "weights2 = [\n",
    "    [0.1, -0.14, -0.14, 0.5],\n",
    "    [-0.5, -0.12, -0.33, -0.5],\n",
    "    [-0.44, 0.73, -0.13, 0.87]\n",
    "]\n",
    "\n",
    "biases2 = [-1, 2, -0.5]\n",
    "\n",
    "resultado_camada1 = np.dot(weights, np.array(inputs).T) + biases\n",
    "\n",
    "resultado_camada2 = np.dot(weights2, np.array(inputs).T) + biases2\n",
    "\n",
    "print(resultado_camada1)\n",
    "\n",
    "print(resultado_camada2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando um objeto de camada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#estamos setando a seed do random para 0\n",
    "np.random.seed(0)\n",
    "\n",
    "X = [[1, 2, 3, 2.5],\n",
    "    [2.0, 5.0, -1.0, 2.0],\n",
    "    [-1.5, 2.7, 3.3, -0.8]]\n",
    "\n",
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        #essa funcao np.random.randn gera números da distribuição normal, e podemos passar o shape do array que ela vai gerar, nesse caso vamos\n",
    "        #pedir para gerar um array com linhas sendo o número de entradas e colunas sendo o número de neurônios\n",
    "        #multiplicamos por 0.1 para normalizar os valores e eles ficarem entre -1 e 1, visto que se não normalizarmos podemos sobrecarregar a rede\n",
    "        #e a capacidade dela\n",
    "        self.weights = 0.1 * np.random.randn(n_inputs, n_neurons)\n",
    "        #aqui nos formamos a matriz de biases para os neuronios, o np.zeros retorna pra gente um array preenchido com 0 no shape que informarmos, no caso,\n",
    "        #são o número de neurônios\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "    def forward(self, inputs):\n",
    "        #aqui temos a saida, fazemos a multiplicação das entradas com seus pesos e acrescentamos os biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saida da camada 1\n",
      "[[-0.12088873  0.43034258  0.53633118 -0.08341587 -0.05415085]\n",
      " [-0.20888481 -0.20167584  0.72514546 -0.33075799  0.2127461 ]\n",
      " [-0.62566662  0.76797946  0.48532311 -0.14053423 -0.40505212]]\n",
      "Saida da camada 2\n",
      "[[-0.08420453 -0.03136665]\n",
      " [-0.08298085  0.057347  ]\n",
      " [-0.17690916 -0.09676833]]\n"
     ]
    }
   ],
   "source": [
    "#aqui criamos duas camadas\n",
    "layer1 = Layer_Dense(4,5)\n",
    "layer2 = Layer_Dense(5,2)\n",
    "\n",
    "#aqui usamos o método para produzir as saidas da primeira camada\n",
    "layer1.forward(X)\n",
    "print(\"Saida da camada 1\")\n",
    "print(layer1.output)\n",
    "print(\"Saida da camada 2\")\n",
    "#aqui usamos a saida da primeira camada e passamos para a segunda camada \n",
    "layer2.forward(layer1.output)\n",
    "print(layer2.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando a função de ativação ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########RESULTADOS SEM FUNCAO DE ATIVACAO RELU#########\n",
      "[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00]\n",
      " [-8.35815910e-04 -7.90404272e-04 -1.33452227e-03  4.65504505e-04\n",
      "   4.56846210e-05]\n",
      " [-2.39994470e-03  5.93469958e-05 -2.24808278e-03  2.03573116e-04\n",
      "   6.10024377e-04]\n",
      " ...\n",
      " [ 1.13291524e-01 -1.89262271e-01 -2.06855070e-02  8.11079666e-02\n",
      "  -6.71350807e-02]\n",
      " [ 1.34588361e-01 -1.43197834e-01  3.09493970e-02  5.66337556e-02\n",
      "  -6.29687458e-02]\n",
      " [ 1.07817926e-01 -2.00809643e-01 -3.37579325e-02  8.72561932e-02\n",
      "  -6.81458861e-02]]\n",
      "#########RESULTADOS COM FUNCAO DE ATIVACAO RELU#########\n",
      "[[0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 4.65504505e-04\n",
      "  4.56846210e-05]\n",
      " [0.00000000e+00 5.93469958e-05 0.00000000e+00 2.03573116e-04\n",
      "  6.10024377e-04]\n",
      " ...\n",
      " [1.13291524e-01 0.00000000e+00 0.00000000e+00 8.11079666e-02\n",
      "  0.00000000e+00]\n",
      " [1.34588361e-01 0.00000000e+00 3.09493970e-02 5.66337556e-02\n",
      "  0.00000000e+00]\n",
      " [1.07817926e-01 0.00000000e+00 0.00000000e+00 8.72561932e-02\n",
      "  0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "\n",
    "#nnfs é uma biblioteca criada pelo proprio canal do curso,\n",
    "#ela tem funcoes que auxiliam no processo de aprendizado de redes neurais\n",
    "#nnfs vem de neural networks from scratch\n",
    "\n",
    "#aqui iniciamos o nnfs\n",
    "nnfs.init()\n",
    "\n",
    "#aqui ele cria dados de exemplo, usando a biblitoeca nnfs\n",
    "X, y = spiral_data(100,3)\n",
    "\n",
    "\n",
    "class Ativacao_ReLu:\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "layer1 = Layer_Dense(2,5)\n",
    "activation1 = Ativacao_ReLu()\n",
    "\n",
    "layer1.forward(X)\n",
    "print('#########RESULTADOS SEM FUNCAO DE ATIVACAO RELU#########')\n",
    "print(layer1.output)\n",
    "print('#########RESULTADOS COM FUNCAO DE ATIVACAO RELU#########')\n",
    "activation1.forward(layer1.output)\n",
    "print(activation1.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando a função de ativação Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saida da camada (Cada linhaa é uma camada) [[12.155]\n",
      " [ 8.7  ]\n",
      " [ 3.157]]\n",
      "valores exponenciais brutos (Cada linhaa é uma camada): [[1.21510418e+02 3.35348465e+00 1.08590627e+01 4.29484260e+01]\n",
      " [7.33197354e+03 1.63654137e-01 1.22140276e+00 4.09595540e+00]\n",
      " [4.09595540e+00 2.86051020e+00 1.02634095e+00 1.95423732e+00]]\n",
      "valores normalizados (Cada linhaa é uma camadacada camada)[[6.80077638e-01 1.87690074e-02 6.07767288e-02 2.40376625e-01]\n",
      " [9.99253009e-01 2.23039387e-05 1.66461373e-04 5.58225659e-04]\n",
      " [4.12190532e-01 2.87863296e-01 1.03284333e-01 1.96661839e-01]]\n",
      "somatorio dos valores normalizados (Cada linhaa é uma camada)[[1.]\n",
      " [1.]\n",
      " [1.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "saidas_camadas = [[4.8, 1.21, 2.385, 3.76],\n",
    "                  [8.9, -1.81, 0.2, 1.41],\n",
    "                  [1.41, 1.051, 0.026, 0.67]\n",
    "                  ]\n",
    "\n",
    "#aqui ele faz o exponencial de todos os valores das saidas das camadas\n",
    "exp_values = np.exp(saidas_camadas)\n",
    "\n",
    "#le as saidas de cada uma das camadas e soma todas elas, por isso o axis=1 (soma todas as linhas)\n",
    "print(f\"saida da camada (Cada linhaa é uma camada) {np.sum(saidas_camadas, axis=1, keepdims=True)}\")\n",
    "\n",
    "#aqui temos os valores exponenciais e brutos, nós não queremos eles, queremos uma distribuição probabilística\n",
    "#nos fizemos o exponencial antes para que não percamos o significado do valor negativo\n",
    "print(f\"valores exponenciais brutos (Cada linhaa é uma camada): {exp_values}\")\n",
    "\n",
    "#aqui eles normalizam os valores, dividindo todos os valores exponenciais brutos pela soma de todos os valores exponenciais brutos\n",
    "norm_values = exp_values / np.sum(exp_values, axis = 1, keepdims=True)\n",
    "\n",
    "print(f\"valores normalizados (Cada linhaa é uma camadacada camada){norm_values}\")\n",
    "\n",
    "#o somatorio tem que dar proximo de 1, porque estamos falando de uma distribuição probabilística\n",
    "print(f\"somatorio dos valores normalizados (Cada linha é uma camada){np.sum(norm_values,axis = 1, keepdims=True)}\")\n",
    "\n",
    "#a ordem do que foi feito seria\n",
    "# entrada -> exponencial -> normalizacao -> saida"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementando as classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33333333 0.33333333 0.33333333]\n",
      " [0.33335356 0.33333025 0.33331619]\n",
      " [0.33333333 0.33333333 0.33333333]\n",
      " [0.3333422  0.33333191 0.33332589]\n",
      " [0.33333333 0.33333333 0.33333333]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "\n",
    "#aqui temos uma classe de camada normal, sem nenhum tipo de função de ativação\n",
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        #essa funcao np.random.randn gera números da distribuição normal, e podemos passar o shape do array que ela vai gerar, nesse caso vamos\n",
    "        #pedir para gerar um array com linhas sendo o número de entradas e colunas sendo o número de neurônios\n",
    "        #multiplicamos por 0.1 para normalizar os valores e eles ficarem entre -1 e 1, visto que se não normalizarmos podemos sobrecarregar a rede\n",
    "        #e a capacidade dela\n",
    "        self.weights = 0.1 * np.random.randn(n_inputs, n_neurons)\n",
    "        #aqui nos formamos a matriz de biases para os neuronios, o np.zeros retorna pra gente um array preenchido com 0 no shape que informarmos, no caso,\n",
    "        #são o número de neurônios\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "    def forward(self, inputs):\n",
    "        #aqui temos a saida, fazemos a multiplicação das entradas com seus pesos e acrescentamos os biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "#aqui temos uma classe da função de ativação ReLU\n",
    "class ativacao_ReLu:\n",
    "    def forward(self, inputs):\n",
    "        #a função de ativação ReLU considera todos os valores negativos e 0 como 0, e se o valor for positivo, considera ele com o o maximo da função\n",
    "        #então se tivermos um valor negativo, ele vai ser 0, se tivermos um valor positivo, ele vai ser ele mesmo, é o que o np.maximum faz\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "class ativacao_softmax:\n",
    "    def forward(self, inputs):\n",
    "        #aqui ele vai fazer o exponencial dos valores, essa subtração do np.max(inputs) é para manter os dados em uma escala pequena, sem isso\n",
    "        #os valores podem explodir e causar overflow, então subtraindo o maior valor de todos os valores, ele garante que os valores vão ser pequenos\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        #aqui normalizamos os valores exponenciais e transformamos eles em probabilidades\n",
    "        probabibilidades = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        self.output = probabibilidades\n",
    "\n",
    "#pegamos os dados de exemplo\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "#criamos a primeira camada com 2 entradas e 3 neurônios\n",
    "camada1 = Layer_Dense(2,3)\n",
    "\n",
    "#colocamos a função de ativação ReLU na primeira camada\n",
    "ativacao1 = ativacao_ReLu()\n",
    "\n",
    "#criamos a segunda camada com 3 entradas e 3 neuronios\n",
    "camada2 = Layer_Dense(3,3)\n",
    "\n",
    "#colocamos a função de ativação softmax na segunda camada\n",
    "ativacao2 = ativacao_softmax()\n",
    "\n",
    "#passamos os dados de entrada para a primeira camada\n",
    "camada1.forward(X)\n",
    "#aplicamos a função de ativação nos resultados da primeira camada\n",
    "ativacao1.forward(camada1.output)\n",
    "\n",
    "#passamos os resultados da primeira camada para a segunda camada\n",
    "camada2.forward(ativacao1.output)\n",
    "#aplicamos a função de ativação nos resultados da segunda camada\n",
    "ativacao2.forward(camada2.output)\n",
    "\n",
    "#printamos as probabilidades\n",
    "print(ativacao2.output[:5])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
